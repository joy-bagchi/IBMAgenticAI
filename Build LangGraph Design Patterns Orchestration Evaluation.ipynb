{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build LangGraph Design Patterns: Orchestration & Evaluation**\n",
    "\n",
    "Estimated time needed: **30** minutes\n",
    "\n",
    "In this hands-on lab, you'll master two fundamental agentic design patterns that power modern AI workflows. You'll build a parallel meal planning system that coordinates multiple AI chefs, and an iterative investment advisor that refines recommendations through continuous feedback loops.\n",
    "\n",
    "**Real world impact**: These patterns are used in production systems at leading companies such as Anthropic, OpenAI, Microsoft, IBM, and Google‚Äîenabling AI agents to collaborate, self-improve, and tackle complex, multi-step reasoning tasks across domains from research to customer service.\n",
    "\n",
    "## Why LangGraph?\n",
    "\n",
    "As AI systems take on more complex problems, a critical challenge arises:  \n",
    "**How can we coordinate multiple agents or components to work collaboratively, intelligently, and adaptively?**\n",
    "\n",
    "This is where **LangGraph** comes in.\n",
    "\n",
    "**LangGraph** is a framework for building **stateful, dynamic, and modular AI workflows**.  \n",
    "\n",
    "Unlike traditional linear chains, LangGraph enables:\n",
    "\n",
    "- üîÑ **Iterative refinement**: Incorporate feedback loops and self-correction logic  \n",
    "- üßµ **Conditional routing**: Dynamically choose different paths based on intermediate results  \n",
    "- ‚ö°Ô∏è **Parallel execution**: Process independent subtasks concurrently to boost efficiency  \n",
    "- üß† **Global state management**: Maintain and evolve shared context across complex workflows\n",
    "\n",
    "With LangGraph, your workflows become more than just pipelines; they become **intelligent systems** capable of reasoning, adapting, and improving over time. This is **ideal** for use cases such as multi-agent collaboration, decision-making, evaluation loops, and planning.\n",
    "\n",
    "We'll be loosely following [this](https://langchain-ai.github.io/langgraph/tutorials/workflows/#orchestrator-worker) LangGraph tutorial throughout this lab.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eXR9pjKDiPrPhLcEjhsKCA/langgraph.png\" width=\"100%\" alt=\"langgraph\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Orchestrator-Worker-Pattern\">Orchestrator-Worker Pattern</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Structured-Output\">Structured Output</a></li>\n",
    "            <li><a href=\"#State-(Orchestration)\">State (Orchestration)</a></li>\n",
    "            <li><a href=\"#Orchestrator-Node\">Orchestrator Node</a></li>\n",
    "            <li><a href=\"#Worker-Nodes\">Worker Nodes</a></li>\n",
    "            <li><a href=\"#Building-the-Workflow-(Orchestration)\">Building the Workflow (Orchestration)</a></li>\n",
    "            <li><a href=\"#Visualization\">Visualization</a></li>\n",
    "            <li><a href=\"#Testing-(Orchestration)\">Testing (Orchestration)</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Reflection-Pattern\">Reflection Pattern</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#State-(Reflection)\">State (Reflection)</a></li>\n",
    "            <li><a href=\"#Setup-Node\">Setup Node</a></li>\n",
    "            <li><a href=\"#Generator-Node\">Generator Node</a></li>\n",
    "            <li><a href=\"#Evaluator-Node\">Evaluator Node</a></li>\n",
    "            <li><a href=\"#Routing-Node\">Routing Node</a></li>\n",
    "            <li><a href=\"#Building-the-Workflow-(Reflection)\">Building the Workflow (Reflection)</a></li>\n",
    "            <li><a href=\"#Testing-(Reflection)\">Testing (Reflection)</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Conclusion\">Conclusion</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Next-Steps\">Next Steps</a></li>\n",
    "            <li><a href=\"#Authors\">Authors</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Design and implement the **Orchestrator-Worker** pattern for parallel task processing.\n",
    "- Build **reflection loops** that enable agents to iteratively improve their outputs.\n",
    "- Manage complex state across **multi-node LangGraph workflows**.\n",
    "- Apply **conditional routing** to create dynamic, intelligent workflows.\n",
    "- Debug and visualize LangGraph execution flows.\n",
    "- Recognize when to use each pattern in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
    "\n",
    "**Note:** The version has been specified here to pin it. It's recommended that you do the same. Even if the library is updated in the future, the installed version will still support this lab work.\n",
    "\n",
    "Since `%%capture` is being used to capture the installation process, you won't see the output. However, once the installation is complete, you will see a number beside the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain-openai==0.3.27\n",
    "!pip install langgraph==0.6.6\n",
    "!pip install pygraphviz==1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "Let's import the required libraries in the following cells starting with our LangGraph modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.types import Send"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for type safety and state management:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for graph visualization of our workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for LangChain API and LLM instantiation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the language model - using gpt-4o-mini for cost-effective experimentation\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrator-Worker Pattern\n",
    "\n",
    "The orchestrator-worker pattern is a powerful approach for handling complex, unpredictable tasks by dynamically **breaking them into manageable pieces** and **processing them in parallel**. When you can't predict upfront how many subtasks you'll need or what they'll involve, this pattern shines by intelligently **analyzing** the input, **decomposing** it into structured work units, and **assigning** specialized workers to handle each piece independently. This pattern is widely used in real-world AI workflow systems, including industry solutions such as IBM‚Äôs [Watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Agentic+Design+Patterns+in+LangGraph-v1_1752612018).\n",
    "\n",
    "Think of it like a busy catering kitchen that takes orders for large events‚Äîwhen a customer orders \"I want a three-course meal, hamburgers for lunch, and pizza for dinner,\" the head chef (orchestrator) analyzes the request and breaks it down into specific dishes, then assigns specialized culinary consultants to create detailed meal plans for each cuisine. Each consultant specializes in a specific cuisine, for example, one expert in Italian cuisine handles the pizza planning, while an American cuisine specialist develops the hamburger menu. The key is that each consultant receives a **clear, structured brief** with dish requirements and dietary considerations, not just \"plan something good.\" In our meal planning system, user requests such as \"feed my family for the week\" could result in anywhere from 3 to 15 different dishes, each requiring a specialist consultant's expertise in their particular cuisine to create comprehensive meal plans.\n",
    "\n",
    "Here's a simple visualization of the workflow (made on [Excalidraw](https://excalidraw.com/)):\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XKXeMzLe9KNW5UjYiJpuSQ/orchestration.png\" width=\"100%\" alt=\"orchestration\">\n",
    "\n",
    "## Structured Output\n",
    "\n",
    "The orchestrator **must** produce **structured outputs** because our worker nodes require specific, well-defined input and output formats to process information reliably. Each worker (chef consultant) specializes in gathering specific information about their assigned cuisine:\n",
    "\n",
    "- Name of the dish (for example, \"Margherita Pizza\", \"Classic Cheeseburger\")\n",
    "- List of ingredients (the most important part for meal planning)\n",
    "- The cuisine or cultural origin of the dish (for example, Italian, American, Mexican)\n",
    "\n",
    "For each worker, we aggregate this data into a structured format called a `Dish`. We can then store a list of dishes with the `Dishes` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dish schema for a single dish\n",
    "class Dish(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name of the dish (for example, Spaghetti Bolognese, Chicken Curry).\"\n",
    "    )\n",
    "    ingredients: List[str] = Field(\n",
    "        description=\"List of ingredients needed for this dish, separated by commas.\"\n",
    "    )\n",
    "    location: str = Field(\n",
    "        description=\"The cuisine or cultural origin of the dish (for example, Italian, Indian, Mexican).\"\n",
    "    )\n",
    "\n",
    "# Dishes schema for a list of Dish objects\n",
    "class Dishes(BaseModel):\n",
    "    sections: List[Dish] = Field(\n",
    "        description=\"A list of grocery sections, one for each dish, with ingredients.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following prompt then transforms the user's meal request into structured recipe cards that individual cuisine specialist workers can reliably process, matching our catering kitchen orchestrator story where the head chef breaks down complex orders into clear, actionable tasks for specialized line cooks. \n",
    "\n",
    "**Note**: You can optionally include the schema field names (name, ingredients, location) in the prompt for extra clarity, but it's not necessary when using structured output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a prompt template\n",
    "dish_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an assistant that generates a structured grocery list.\\n\\n\"\n",
    "        \"The user wants to prepare the following meals: {meals}\\n\\n\"\n",
    "        \"For each meal, return a section with:\\n\"\n",
    "        \"- the name of the dish\\n\"\n",
    "        \"- a comma-separated list of ingredients needed for that dish.\\n\"\n",
    "        \"- the cuisine or cultural origin of the food\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the planner, we chain the prompt to the LLM and use `with_structured_output()` to format the output with the `Dishes` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dishes(sections=[Dish(name='banana smoothie', ingredients=['bananas', 'milk', 'yogurt', 'honey', 'ice'], location='American'), Dish(name='carrot cake', ingredients=['carrots', 'flour', 'sugar', 'eggs', 'baking powder', 'cinnamon', 'vegetable oil', 'cream cheese', 'walnuts'], location='American')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use LCEL to pipe the prompt to an LLM with a structured output of Dishes\n",
    "planner_pipe = dish_prompt | llm.with_structured_output(Dishes)\n",
    "\n",
    "# invoke the planner_pipe with example meals\n",
    "planner_pipe.invoke({ \"meals\" : [\"banana smoothie\", \"carrot cake\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a processing pipeline where the prompt guides the LLM's reasoning, and `with_structured_output(Dishes)` ensures the response automatically conforms to our defined Pydantic schema structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State (Orchestration)\n",
    "\n",
    "In LangGraph, **state** is the shared memory that flows through your workflow. It captures everything the agents or nodes need to know, modify, or pass along to the next step.\n",
    "\n",
    "Think of it as the **context backpack** carried from node to node.\n",
    "\n",
    "Unlike a traditional chain where only input and output are passed along, LangGraph gives each node access to a structured `state` dictionary, which can hold:\n",
    "\n",
    "- The original user input or goal.\n",
    "- Intermediate results (for example, parsed sections, completed plans).\n",
    "- Final summaries or evaluations.\n",
    "- Data for decision-making or routing (for example, risk level, retry count).\n",
    "\n",
    "### Why State Matters in Agentic Workflows\n",
    "\n",
    "Agentic systems often involve multiple steps, roles, and decision points. Having a shared, evolving state lets your agents:\n",
    "\n",
    "- Access relevant context from earlier steps.\n",
    "- Modify or enrich the state as they work.\n",
    "- Route logic dynamically based on conditions.\n",
    "- Loop, retry, or reflect based on what‚Äôs in the state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    meals: str  # The user's input listing the meals to prepare\n",
    "    sections: List[Dish] # One section per meal/dish with ingredients\n",
    "    completed_menu: Annotated[List[str], operator.add]  # Worker written dish guide chunks\n",
    "    final_meal_guide: str  # Fully compiled, readable menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `meals` is the initial input from the user\n",
    "- `sections` will be the structured list of each dish created by the orchestrator\n",
    "- `completed_menu` will be the output from each worker (meal specialist), with `operator.add` automatically aggregating results from all parallel workers\n",
    "- `final_meal_guide` will be the final aggregation of all worker outputs into a complete meal planning guide\n",
    "\n",
    "Let's test the planner LLM before we create a LangGraph node. We initialize a dummy state with all fields blank except the meals key, then invoke the planner to see how it structures the user input into organized dish sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy state to test our worker nodes\n",
    "dummy_state: State = {\n",
    "    \"meals\": \"Spaghetti Bolognese and Chicken Stir Fry\",\n",
    "    \"sections\": [],\n",
    "    \"completed_menu\": [],\n",
    "    \"final_meal_guide\": \"\"\n",
    "}\n",
    "\n",
    "report_sections = planner_pipe.invoke({\"meals\": dummy_state['meals']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the output of the LLM to see its structure. The result is a `Dishes` object containing a `sections` attribute, which is essentially a list of `Dish` objects with `name`, `ingredients`, and `location` attributes filled out for each dish (Italian and Chinese food respectively). We can iterate through and print them out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dish 1\n",
      "\n",
      "Item Name: Spaghetti Bolognese\n",
      "Location/Cuisine: Italian\n",
      "Ingredients: spaghetti, ground beef, onion, garlic, carrot, celery, canned tomatoes, tomato paste, olive oil, basil, oregano, salt, pepper.\n",
      "Dish 2\n",
      "\n",
      "Item Name: Chicken Stir Fry\n",
      "Location/Cuisine: Chinese\n",
      "Ingredients: chicken breast, bell peppers, broccoli, carrots, soy sauce, ginger, garlic, sesame oil, green onions, rice.\n"
     ]
    }
   ],
   "source": [
    "for i, section in enumerate(report_sections.sections):\n",
    "    print(f\"Dish {i+1}\\n\")\n",
    "    # add each dish to our dummy state\n",
    "    dummy_state[\"sections\"].append(section)\n",
    "    print(f\"Item Name: {section.name}\")\n",
    "    print(f\"Location/Cuisine: {section.location}\")\n",
    "    print(f\"Ingredients: {\", \".join(section.ingredients)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator Node\n",
    "\n",
    "The **orchestrator** is responsible for high-level planning and acts as the central coordinator in the LangGraph workflow. It takes a user's input and produces structured subtasks for other nodes to handle.\n",
    "\n",
    "In our workflow, the orchestrator:\n",
    "\n",
    "- Takes the raw input (for example, `\"Spaghetti Bolognese and Chicken Stir Fry\"`).\n",
    "- Uses an LLM to break it down into structured `Dish` objects.\n",
    "- Returns the result as a dict with the field `sections` for worker nodes to process.\n",
    "\n",
    "This enables **fan-out parallelism**, where multiple workers can now act independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a structured dish list from the given meals.\"\"\"\n",
    "\n",
    "    # use the planner_pipe LLM to break the user's meal list into structured dish sections\n",
    "    dish_descriptions = planner_pipe.invoke({\"meals\": state[\"meals\"]})\n",
    "\n",
    "    # return the list of dish sections to be passed to worker nodes\n",
    "    return {\"sections\": dish_descriptions.sections}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker Nodes\n",
    "\n",
    "Each **worker node** processes a single unit of work, typically one item from `state[\"sections\"]`.\n",
    "\n",
    "In this case, each worker will be a chef. The prompt template's system message specifies the input using the outputs from the orchestrator's schema. For example, a worker will:\n",
    "\n",
    "- Read one `Dish` object (for example, `\"Spaghetti Bolognese\"`).\n",
    "- Use an LLM to generate a detailed recipe or instructions.\n",
    "- Append the result to `state[\"completed_menu\"]`.\n",
    "\n",
    "Workers are designed to be **isolated** so they only see a slice of the state and return an output.\n",
    "\n",
    "We'll first create the prompt to get a guide for each dish from a professional chef:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a world-class chef from {location}.\\n\\n\"\n",
    "        \"Please introduce yourself briefly and present a detailed walkthrough for preparing the dish: {name}.\\n\"\n",
    "        \"Your response should include:\\n\"\n",
    "        \"- Start with hello with your  name and culinary background\\n\"\n",
    "        \"- A clear list of preparation steps\\n\"\n",
    "        \"- A full explanation of the cooking process\\n\\n\"\n",
    "        \"Use the following ingredients: {ingredients}.\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then chain the prompt to the LLM, creating a `chef_llm`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_pipe = chef_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WorkerState` defines the shape of the state that each **individual chef worker node** receives and returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerState(TypedDict):\n",
    "    section: Dish\n",
    "    completed_menu: Annotated[list, operator.add] # list with addition operators between elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `assign_workers` function extracts each `Dish` object from the `sections` key and passes it to the worker node `chef_worker` that we have not defined yet. It does this by first extracting each section `s` from the `sections` value in the state variable. Then the `Send()` function passes `s` to the `chef_worker` node via the `section` key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"chef_worker\", {\"section\": s}) for s in state[\"sections\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap the `chef_llm` as a worker node called `chef_worker`. Each worker is responsible for taking one dish section, provided through the `assign_workers` via `WorkerState`, and generating a detailed cooking plan for that meal.\n",
    "\n",
    "The inputs are passed via the worker state through the `section` variable that is of type `Dish`, which the `assign_workers` node extracts from the orchestrator node and passes to the LLM.\n",
    "\n",
    "The outputs are saved to `completed_menu`. Since state and worker share this value, the main State is automatically updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chef_worker(state: WorkerState):\n",
    "    \"\"\"Worker node that generates the cooking instructions for one meal section.\"\"\"\n",
    "\n",
    "    # Use the language model to generate a meal preparation plan\n",
    "    # The model receives the dish name, location, and ingredients from the current section\n",
    "    meal_plan = chef_pipe.invoke({\n",
    "        \"name\": state[\"section\"].name,\n",
    "        \"location\": state[\"section\"].location,\n",
    "        \"ingredients\": state[\"section\"].ingredients\n",
    "    })\n",
    "\n",
    "    # Return the generated meal plan wrapped in a list under completed_sections\n",
    "    # This will be merged into the main state using operator.add in LangGraph\n",
    "    return {\"completed_menu\": [meal_plan.content]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replicate what's happening with a 'for' loop, iterating through the dummy worker state sections and passing each section to the chef LLM, then appending the results to `completed_menu`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dishes: List[Dish] = dummy_state[\"sections\"]\n",
    "\n",
    "# simulate LangGraph's fan-out and merging behavior\n",
    "for section in dummy_dishes:\n",
    "    # construct individual WorkerState\n",
    "    worker_state: WorkerState = {\n",
    "        \"section\": section,\n",
    "        \"recipe\": []  # LangGraph merges this later\n",
    "    }\n",
    "\n",
    "    # call the worker logic directly\n",
    "    result = chef_worker(worker_state)\n",
    "\n",
    "    # merge the result into combined menu (LangGraph would do this with operator.add)\n",
    "    dummy_state[\"completed_menu\"] += result[\"completed_menu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the final output, we simply concatenate the elements from `completed_menu` and print out the first 1000 characters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buongiorno! My name is Chef Marco, and I have dedicated over 20 years of my life to mastering the art of Italian cuisine. Growing up in the heart of Tuscany, I was inspired by my Nonna's traditional recipes and the rich flavors of our land. Today, I am excited to share with you one of my favorite dishes: Spaghetti Bolognese. This classic Italian dish is loved worldwide for its hearty flavors and comforting essence. Let's dive into the preparation and cooking process!\n",
      "\n",
      "### Ingredients:\n",
      "- 400g spaghetti\n",
      "- 500g ground beef\n",
      "- 1 medium onion, finely chopped\n",
      "- 2 cloves garlic, minced\n",
      "- 1 medium carrot, finely diced\n",
      "- 1 stalk celery, finely diced\n",
      "- 400g canned tomatoes (diced or crushed)\n",
      "- 2 tablespoons tomato paste\n",
      "- 3 tablespoons olive oil\n",
      "- 1 teaspoon dried basil\n",
      "- 1 teaspoon dried oregano\n",
      "- Salt and pepper to taste\n",
      "- Grated Parmesan cheese (optional, for serving)\n",
      "\n",
      "### Preparation Steps:\n",
      "\n",
      "1. **Prepare the Vegetables:**\n",
      "   - Finely chop the onion, carrot, and celery.\n",
      "   - Mince the garlic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completed_menu_sections = \"\\n\".join(dummy_state[\"completed_menu\"])\n",
    "print(completed_menu_sections[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the process above to a **synthesizer** node in LangGraph. The synthesizer node takes responses from all worker nodes and aggregates them into a final workflow output.\n",
    "\n",
    "The output will populate the `final_meal_guide` key:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # list of completed sections\n",
    "    completed_sections = state[\"completed_menu\"]\n",
    "\n",
    "    # format completed section to str to use as context for final sections\n",
    "    completed_menu = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_meal_guide\": completed_menu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Workflow (Orchestration)\n",
    "\n",
    "To build an agentic workflow in LangGraph, we use a `StateGraph`, a flexible graph structure that coordinates how nodes process and update shared state.\n",
    "\n",
    "Each node in the graph represents a **modular computation step**, such as planning, generating, evaluating, or synthesizing. The `StateGraph` defines how those nodes are connected and how data (state) flows between them. In a `StateGraph`, the input state travels through the workflow getting updated at each state, this is why the naming convention of the state remains consistent throughout the workflow.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize the graph** with a `State` schema that defines what information will persist and evolve across the workflow.\n",
    "2. **Add nodes** using `.add_node(name, function)`, where each node operates on a portion of the state.\n",
    "    - `orchestrator`, `chef_worker`, and `synthesizer`\n",
    "4. **Connect the nodes** using:\n",
    "   - `.add_edge(from_node, to_node)` for simple sequential steps\n",
    "   - `.add_conditional_edges(...)` for dynamic routing or parallel fan-out\n",
    "5. **Compile the graph** to finalize structure and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7adcbbf74d10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the builder\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"chef_worker\", chef_worker)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add a conditional edge from the `orchestrator` node to `chef_worker`, which will create a variable number of nodes from the `assign_workers` function. `orchestrator` is the source node, `assign_workers` is the special routing function we created, and `chef_worker` is the target worker node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7adcbbf74d10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"chef_worker\"] # source node, routing function, list of allowed targets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the entry point by connecting `START` to the orchestrator node, which will **begin** the meal planning process when the workflow is invoked. We also connect all worker outputs to the synthesizer node and then connect the synthesizer to the `END` node to **complete** the workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7adcbbf74d10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the edges, connections between nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_edge(\"chef_worker\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the workflow builder into an executable graph that can process our meal planning requests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the builder to get a complete workflow executable\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "LangGraph provides built-in support for visualizing workflows using [**Mermaid**](https://docs.mermaidchart.com/mermaid/intro), a lightweight diagramming syntax.\n",
    "\n",
    "The Mermaid chart helps you:\n",
    "\n",
    "- See how nodes (for example, `orchestrator`, `chef_worker`, `synthesizer`) are connected. \n",
    "- Identify loops, fan-outs, or conditional flows.\n",
    "- Understand how state moves through the workflow.\n",
    "\n",
    "This is especially useful for debugging, communicating workflow logic, or validating the structure of complex agentic systems before running them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the orchestrator workflow using mermaid chart\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing (Orchestration)\n",
    "\n",
    "Finally, we'll test the workflow by invoking it with a sample meal request to see the end-to-end meal planning process in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the workflow with a string of meals in a dict\n",
    "state = orchestrator_worker.invoke({\"meals\": \"Steak and eggs, tacos, and chili\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the complete meal planning guide that contains all the detailed cooking instructions from our specialized chefs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 2000 characters of our final_meal_guide\n",
    "pprint(state[\"final_meal_guide\"][:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection Pattern\n",
    "\n",
    "The Reflection Pattern is where an AI agent **iteratively** evaluates and improves its own outputs through self-assessment and feedback loops, similar to reflective agents in cognitive science. Rather than providing a single recommendation, the system creates an initial proposal, critically evaluates it against established criteria, and then refines the output based on that evaluation. This cycle continues **until the system achieves an optimal solution** that meets predefined quality standards.\n",
    "\n",
    "In this section, we'll build an intelligent investment advisory system that iteratively refines portfolio recommendations through continuous evaluation and feedback loops. The system combines aggressive growth strategies with conservative risk assessment to find the optimal balance for each investor's profile. The reflection pattern enables the system to self-correct and adapt its recommendations, ensuring each portfolio suggestion is thoroughly vetted before presentation to the investor.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9kH761sRFhuVPWzBUZhgkA/reflection.png\" width=\"100%\" alt=\"reflection\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State (Reflection)\n",
    "\n",
    "Similar to the previous workflow, we'll also be using a state to track variables and values across this workflow. The following `State` dictionary represents the evolving context in an **Investment Advice Evaluator-Optimizer workflow**, tracking the user's investment profile, their investment plan, feedback from the evaluator, the user's safety (or risk) profile, the target safety profile, and a counter `n` for iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = Literal[\n",
    "    \"ultra-conservative\", \n",
    "    \"conservative\", \n",
    "    \"moderate\", \n",
    "    \"aggressive\", \n",
    "    \"high risk\"\n",
    "]\n",
    "\n",
    "class State(TypedDict):\n",
    "    investment_plan: str\n",
    "    investor_profile: str\n",
    "    target_grade: grades\n",
    "    feedback: str\n",
    "    grade: grades\n",
    "    n: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `investment_plan`: Our generated plan that will be evaluated and revised if need be\n",
    "- `investor_profile`: The user inputted profile that we'll use as a reference for the plan\n",
    "- `target_grade`: A generated ideal risk tolerance grade based on the investor profile\n",
    "- `feedback`: Evaluator feedback for the investment plan\n",
    "- `grade`: Evaluated grade of the investment plan\n",
    "- `n`: Number of evaluation iterations\n",
    "\n",
    "**Note:** Both grade fields are `Literal` meaning they can only take a value within the defined list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Node\n",
    "\n",
    "Before building our Generator and Evaluator nodes, let's create a node that determines a fitting `target_grade` based on the investor profile. This value will remain **static** throughout the evaluation workflow and exists to push the generated output towards a general direction.\n",
    "\n",
    "We'll start by creating the prompt and the pipe to get the `target_grade`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are an investment advisor. Given the investor‚Äôs profile and their proposed plan,\"\n",
    "     \"choose exactly one risk classification from: ultra-conservative, conservative, moderate, aggressive, high risk.\"\n",
    "     \"Return ONLY the grade.\"\n",
    "    ),\n",
    "    (\"user\",\n",
    "     \"Investor profile:\\n\\n{investor_profile}\\n\\n\"\n",
    "    )\n",
    "])\n",
    "\n",
    "grade_pipe = grade_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pipe we can create a function (node) to return the dict with the ideal `target_grade` key:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_target_grade(state: State):\n",
    "    \"\"\"Ask the LLM to pick the best-fitting target_grade.\"\"\"\n",
    "    response = grade_pipe.invoke({\n",
    "        \"investor_profile\": state[\"investor_profile\"]\n",
    "    })\n",
    "    \n",
    "    # return as a plain dict so LangGraph can merge it into the state\n",
    "    return {\"target_grade\": response.content.lower()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the **orchestration workflow** we'll use a `dummy_state` to test our nodes. Let's initialize it with an example investor profile:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty state except for the user inputted investor profile\n",
    "dummy_state: State = {\n",
    "    \"investment_plan\": \"\",\n",
    "    \"investor_profile\": (\n",
    "        \"Age: 29\\n\"\n",
    "        \"Salary: $110,000\\n\"\n",
    "        \"Assets: $40,000\\n\"\n",
    "        \"Goal: Achieve financial independence by age 45\\n\"\n",
    "        \"Risk tolerance: High\"\n",
    "    ),\n",
    "    \"target_grade\": \"\",\n",
    "    \"feedback\": \"\",\n",
    "    \"grade\": \"\",\n",
    "    \"n\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function we can determine a target grade and update the dummy state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target grade\n",
    "target_grade = determine_target_grade(dummy_state)\n",
    "# update target grade with the returned dict\n",
    "dummy_state.update(target_grade)\n",
    "pprint(dummy_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Node\n",
    "\n",
    "The first step in the Reflection workflow is the **generator node**, which produces an investment plan based on an investor's profile. The generator node is responsible for producing an investment strategy based on the investor profile. However, instead of generating a single plan, we design a **multi-phase generator** that evolves through feedback: starting bold, then refining based on evaluation.\n",
    "\n",
    "### Phase 1: Initial Generation\n",
    "\n",
    "In this initial generated strategy, we simulate the style of **Cathie Wood**, a well-known investor who takes a venture-style approach: favoring high-growth, high-risk opportunities over conservative, value-based strategies.\n",
    "\n",
    "To model this, we define:\n",
    "\n",
    "- A **system message** that describes Cathie Wood's investment philosophy: bold, tech-focused, and speculative.\n",
    "- A **user message** that provides the specific investor profile.\n",
    "- A **chained LLM**, `cathie_wood_pipe`, which combines both messages into a structured generation pipeline.\n",
    "\n",
    "This generator node serves as the **creative initiator** of the workflow, producing a candidate investment strategy that downstream nodes will later evaluate, critique, or refine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inital generation, no feedback, only based on profile\n",
    "cathie_wood_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "    \"\"\"You are a bold, innovation-driven investment advisor inspired by Cathie Wood.\n",
    "\n",
    "Your goal is to generate a high-conviction, forward-looking investment plan that embraces disruptive technologies,\n",
    "emerging markets, and long-term growth potential. You are not afraid of short-term volatility as long as the upside is transformational.\n",
    "\n",
    "Create an investment strategy tailored to the investor profile below. Prioritize innovation and high-reward opportunities,\n",
    "such as artificial intelligence, biotechnology, blockchain, or renewable energy.\n",
    "\n",
    "Respond with a concise investment plan in paragraph form.\n",
    "\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"Investor profile:\\n\\n{investor_profile}\")\n",
    "])\n",
    "\n",
    "cathie_wood_pipe = cathie_wood_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second phase of the generator will require feedback from the evaluator, so let's format the evaluator's output for subsequent generation. After the initial plan is generated, an evaluator agent will analyze it and return a structured `Feedback` schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator output schema\n",
    "class Feedback(BaseModel):\n",
    "    grade: grades = Field(\n",
    "        description=\"Classify the investment based on risk level, ranging from ultra-conservative to high risk.\"\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Provide reasoning for the risk classification assigned to the investment suggestion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Adaptive Generation\n",
    "\n",
    "To improve the initial plan, we introduce a second generator ‚Äî this time inspired by **Ray Dalio**, a more conservative and macroeconomically grounded investor. This part takes the `Feedback` (`grade` and `feedback`) from the **evaluator** along with the `investor_profile` from the state. This is done via the **human message** as the LLM is imagining a conversation where it has received previous feedback.\n",
    "\n",
    "To model this, we define:\n",
    "\n",
    "- A **system message** that describes Ray Dalio's investment strategy: conservative.\n",
    "- A **human message** that provides the specific investor profile and the feedback.\n",
    "- A **chained LLM**, `ray_dalio_pipe`, which combines both messages into a structured generation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_dalio_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "    \"\"\"You are an investment advisor inspired by Ray Dalio's principles but with adaptive strategy generation.\n",
    "Your goal is to create varied, scenario-aware investment plans that respond dynamically to economic conditions,\n",
    "feedback, and the investor's evolving needs. You adapt your recommendations based on previous evaluations.\n",
    "\n",
    "CORE PRINCIPLES:\n",
    "- Environmental diversification across economic regimes (growth/inflation combinations)\n",
    "- Risk parity weighting by volatility, not just dollar amounts\n",
    "- Inflation-aware asset selection with real return focus\n",
    "- Macroeconomic scenario planning and regime identification\n",
    "\n",
    "ADAPTATION RULES based on feedback:\n",
    "- If deemed \"too conservative\" ‚Üí Increase growth equity allocation, add emerging markets, consider alternatives\n",
    "- If deemed \"too aggressive\" ‚Üí Add defensive assets, increase bond allocation, focus on dividend stocks\n",
    "- If \"lacks inflation protection\" ‚Üí Emphasize TIPS, commodities, REITs, international exposure\n",
    "- If \"too complex\" ‚Üí Simplify to core ETF strategy with clear rationale\n",
    "- If \"insufficient diversification\" ‚Üí Add geographic, sector, or alternative asset exposure\n",
    "\n",
    "ECONOMIC SCENARIO ADJUSTMENTS:\n",
    "- Rising inflation environment ‚Üí Emphasize commodities, TIPS, real estate, reduce duration\n",
    "- Stagflation concerns ‚Üí Focus on energy, materials, international markets, inflation hedges\n",
    "- Deflationary risks ‚Üí Increase government bonds, high-quality corporate bonds, cash positions\n",
    "- Growth acceleration ‚Üí Favor technology, consumer discretionary, small-cap growth\n",
    "- Economic uncertainty ‚Üí Balance with \"All Weather\" approach using multiple asset classes\n",
    "\n",
    "TARGETING 15% RETURNS through:\n",
    "- Strategic overweighting of growth assets during favorable conditions\n",
    "- Tactical allocation adjustments based on economic regime\n",
    "- Alternative investments (REITs, commodities, international) for diversification\n",
    "- Leverage consideration for qualified investors\n",
    "- Regular rebalancing to capture volatility\n",
    "\n",
    "Respond with a clear, actionable investment plan that reflects current economic conditions \n",
    "and adapts to the specific feedback provided. Vary your approach significantly based on \n",
    "the grade and feedback received.\n",
    "\"\"\"\n",
    "    ),\n",
    "    (\"human\",\n",
    "     \"\"\"Investor profile:\n",
    "{investor_profile}\n",
    "\n",
    "Previous strategy grade: {grade}\n",
    "\n",
    "Evaluator feedback: {feedback}\n",
    "\n",
    "Based on this feedback, create a NEW investment strategy that addresses the concerns raised \n",
    "while targeting 15% returns. Make significant adjustments from any previous approach.\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "ray_dalio_pipe = ray_dalio_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Generator Node\n",
    "\n",
    "The `investment_plan_generator` node serves as a dynamic strategy creator in our **Reflection workflow**. It adapts its behavior based on whether the system is generating an initial plan or refining a prior one based on feedback.\n",
    "\n",
    "This generator switches between two distinct investment personas:\n",
    "\n",
    "- **Cathie Wood‚Äìstyle (Initial Phase)**  \n",
    "  If no prior feedback exists in the state, the generator uses `cathie_wood_pipe`, producing a high-growth, innovation-driven strategy that emphasizes disruptive technologies and bold returns.\n",
    "\n",
    "- **Ray Dalio‚Äìstyle (Refinement Phase)**  \n",
    "  If feedback from an evaluator is already present (`state[\"feedback\"]`), the generator invokes `ray_dalio_pipe` to adapt the plan accordingly. It uses:\n",
    "  - The original investor profile\n",
    "  - The evaluator's risk grade (for example, \"high risk\")\n",
    "  - Specific feedback (for example, \"not diversified\", \"too aggressive\")\n",
    "\n",
    "This allows the workflow to **evolve investment strategies over time**, adjusting recommendations in response to critique ‚Äî a key agentic reflection pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investment_plan_generator(state: State) -> dict:\n",
    "    \"\"\"Prompts an LLM to generate or improve an investment plan based on the current state.\"\"\"\n",
    "\n",
    "    if state.get(\"feedback\"):\n",
    "        # use Ray Dalio‚Äìstyle generator when feedback is available\n",
    "        response = ray_dalio_pipe.invoke({\n",
    "            \"investor_profile\": state[\"investor_profile\"],\n",
    "            \"grade\": state[\"grade\"],\n",
    "            \"feedback\": state[\"feedback\"]\n",
    "        })\n",
    "    else:\n",
    "        # use Cathie Wood‚Äìstyle generator for initial plan\n",
    "        response = cathie_wood_pipe.invoke({\n",
    "            \"investor_profile\": state[\"investor_profile\"]\n",
    "        })\n",
    "\n",
    "    return {\"investment_plan\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the generator node with our `dummy_state`. Currently it only has the `investment_profile` and `target_grade` fields filled and the `feedback` field empty so it will go with the Cathie Wood-style investment plan generator. We can update the `dummy_state` variable with the new plan and print it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the investment plan\n",
    "initial_investment_plan = investment_plan_generator(dummy_state)\n",
    "# update the dummy state with generated plan\n",
    "dummy_state.update(initial_investment_plan)\n",
    "pprint(dummy_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Node\n",
    "\n",
    "The Evaluator Node is responsible for critically assessing the investment strategy generated by the system. It does so through the lens of **Warren Buffett's value-investing philosophy**, which emphasizes:\n",
    "\n",
    "- Capital preservation  \n",
    "- Sound business fundamentals  \n",
    "- Long-term stability  \n",
    "- Caution toward speculative or high-volatility assets\n",
    "\n",
    "This makes Buffett's conservative worldview the perfect counterbalance to the aggressive strategies generated by the Cathie Wood‚Äìstyle generator.\n",
    "\n",
    "### What the Evaluator Does\n",
    "\n",
    "The evaluator reviews the generated `investment_plan` in the context of the `investor_profile` and a predefined `target_grade`. It then:\n",
    "\n",
    "- Assigns a risk **grade** (One of the predefined `Literal` values).\n",
    "- Provides a concise **feedback explanation** justifying the grade.\n",
    "\n",
    "The evaluation schema `Feedback` was defined earlier, it'll be used to defined the structured output for our `evaluator_prompt`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warren Buffet style evaluation prompt\n",
    "evaluator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\"You are an investment risk evaluator inspired by Warren Buffett's value investing philosophy.\n",
    "\n",
    "Your task is to assess whether a proposed investment strategy aligns with conservative, value-driven principles \n",
    "that emphasize capital preservation, long-term stability, and sound business fundamentals. You should be \n",
    "skeptical of speculative investments, high-volatility assets, and short-term market trends.\n",
    "\n",
    "RISK CLASSIFICATION LEVELS:\n",
    "- ultra-conservative: Extremely safe, minimal risk of loss\n",
    "- conservative: Low risk, prioritizes capital preservation  \n",
    "- moderate: Balanced approach with acceptable risk-reward ratio\n",
    "- aggressive: Higher risk for potentially greater returns\n",
    "- high risk: Speculative investments with significant loss potential\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "- Business clarity: Is the investment easily understandable with transparent cash flows?\n",
    "- Margin of safety: Does the investment price provide protection against downside risk?\n",
    "- Capital preservation: Will this strategy protect wealth over the long term?\n",
    "- Investor alignment: Does this match a conservative investor's risk tolerance and goals?\n",
    "- Quality fundamentals: Are the underlying assets financially sound with competitive advantages?\n",
    "\n",
    "Return your assessment in the following  format:\n",
    "{{\n",
    "  \"grade\": \"<investment risk level>\",\n",
    "  \"feedback\": \"<concise explanation of the assigned risk level and key reasoning>\"\n",
    "}}\n",
    "\"\"\"\n",
    "    ),\n",
    "    (\"human\", \n",
    "     \"Evaluate this investment plan:\\n\\n{investment_plan}\\n\\nFor this investor profile:\\n\\n{investor_profile}\\n\\nAnd provide feedback that matches this target risk level: {target_grade}\")\n",
    "])\n",
    "\n",
    "# create the pipe with the structured output that outputs a Feedback object\n",
    "buffett_evaluator_pipe = evaluator_prompt | llm.with_structured_output(Feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Generator Node\n",
    "\n",
    "The `evaluate_plan` node is responsible for reviewing the generated investment strategy and assigning it a risk grade and explanation. This is done using our **Buffett-inspired evaluator** pipeline, which applies value-investing principles to judge whether the proposed plan matches the investor's target risk profile.\n",
    "\n",
    "This node:\n",
    "\n",
    "- Takes the current `investment_plan` from the state\n",
    "- Assesses it with regards to the `investor_profile` and your ideal `target_grade`\n",
    "- Uses the `buffett_evaluator` to generate:\n",
    "  - A structured **grade** (for example, `\"moderate\"`, `\"aggressive\"`)\n",
    "  - A **feedback** string justifying the risk classification\n",
    "- Increments a counter `n` to keep track of how many iterations (reflection loops) have occurred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_plan(state: State):\n",
    "    \"\"\"LLM evaluates the investment plan\"\"\"\n",
    "\n",
    "    # add one to the current count\n",
    "    current_count = state.get('n', 0) + 1\n",
    "\n",
    "    # get the evaluation result from the evaluator pipe\n",
    "    evaluation_result = buffett_evaluator_pipe.invoke({\n",
    "        \"investment_plan\": state[\"investment_plan\"],\n",
    "        \"investor_profile\": state[\"investor_profile\"],\n",
    "        \"target_grade\": state[\"target_grade\"]\n",
    "    })\n",
    "\n",
    "    # return the grade and feedback in a dict\n",
    "    return {\"grade\": evaluation_result.grade, \"feedback\": evaluation_result.feedback, \"n\": current_count}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the evaluator node with our `dummy_state`. Pass the state into our `evaluate_plan` to evaluate the investment plan and update the state with the returned dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feedback\n",
    "evaluated_feedback = evaluate_plan(dummy_state)\n",
    "# update the dummy state with the feedback\n",
    "dummy_state.update(evaluated_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the `feedback` and `grade` of our investment plan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Grade: {dummy_state[\"grade\"]}\")\n",
    "print(f\"Feedback: {dummy_state[\"feedback\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing Node\n",
    "\n",
    "The `route_investment` node acts as a **decision-making mechanism** in our LangGraph workflow. After a strategy is evaluated, this node determines whether to:\n",
    "\n",
    "- Accept the current investment plan and end the loop.\n",
    "- Send it back to the generator for refinement.\n",
    "- Stop the process entirely after too many failed attempts.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The router checks two key fields from the shared `state`:\n",
    "\n",
    "1. `grade`: Assigned by the evaluator\n",
    "2. `target_grade`: The prefferred risk classification from the investor profile\n",
    "\n",
    "It also checks the **iteration count** `n`, which tracks how many times the strategy has been revised:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_investment(state: State, iteration_limit: int = 5):\n",
    "    \"\"\"Route investment based on risk grade evaluation\"\"\"\n",
    "    # get grades\n",
    "    current_grade = state.get(\"grade\", \"MISSING\")\n",
    "    target_grade = state.get(\"target_grade\", \"MISSING\")\n",
    "    # check if grades match\n",
    "    match = current_grade == target_grade\n",
    "\n",
    "    # print out the tracked values\n",
    "    print(f\"=== ROUTING  ===\")\n",
    "    print(f\"Current grade: '{current_grade}'\")\n",
    "    print(f\"Target risk profile: '{target_grade}'\")\n",
    "    print(f\"Match: {match}\")\n",
    "    print(f\"Number of trials: {state['n']}\")\n",
    "\n",
    "    # routing logic\n",
    "    if match: # grades match\n",
    "        print(\"‚Üí Routing to: Accepted\")\n",
    "        return \"Accepted\"\n",
    "    elif state['n'] > iteration_limit: # review iterations exceeds limit\n",
    "        print(\"‚Üí Too many iterations, stopping\")\n",
    "        return \"Accepted\"\n",
    "    else: # grades don't match\n",
    "        print(\"‚Üí Routing to: Rejected + Feedback\")\n",
    "        return \"Rejected + Feedback\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Workflow (Reflection)\n",
    "\n",
    "To enable iterative refinement of investment strategies, we construct a **Reflection Workflow** using `StateGraph`. This workflow mimics how a thoughtful advisor might revise and re-evaluate strategies over multiple rounds based on feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "The reflection pattern involves a **loop** between generation and evaluation:\n",
    "\n",
    "1. Generate a strategy based on the investor profile and risk appetite\n",
    "2. Evaluate the strategy using structured feedback\n",
    "3. If the strategy doesn't match the target grade, revise and repeat\n",
    "\n",
    "This cycle allows the system to **improve** and **adapt** the investment plan through reflection ‚Äî not just respond once.\n",
    "\n",
    "---\n",
    "\n",
    "### Building\n",
    "\n",
    "1. **Add Core Nodes**\n",
    "   - `determine_target_grade`: Determines the appropriate target risk level based on the investor profile.\n",
    "   - `investment_plan_generator`: Generates an investment plan using an LLM in the style of Cathie Wood or Ray Dalio.\n",
    "   - `evaluate_plan`: Evaluates the generated plan using a Buffett-inspired risk assessment model.\n",
    "2. **Define the Workflow Edges**\n",
    "   - Connect `START ‚Üí determine_target_grade`\n",
    "   - Connect `determine_target_grade ‚Üí investment_plan_generator`\n",
    "   - Connect `investment_plan_generator ‚Üí evaluate_plan`\n",
    "3. **Establish Core Flow**\n",
    "   - The generator and evaluator form the backbone of the reflection loop.\n",
    "   - This flow ensures that every strategy is generated and evaluated before further routing decisions are made.\n",
    "4. Add a **conditional Routing Node** to:\n",
    "   - Compare the evaluated grade to the target grade.\n",
    "   - Decide whether to accept the strategy or route it back for refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize StateGraph with the given State schema\n",
    "optimizer_builder = StateGraph(State)\n",
    "\n",
    "# add the setup, generator, and evaluator nodes\n",
    "optimizer_builder.add_node(\"determine_target_grade\", determine_target_grade)\n",
    "optimizer_builder.add_node(\"investment_plan_generator\", investment_plan_generator)\n",
    "optimizer_builder.add_node(\"evaluate_plan\", evaluate_plan)\n",
    "\n",
    "# define the flow with edges\n",
    "optimizer_builder.add_edge(START, \"determine_target_grade\")\n",
    "optimizer_builder.add_edge(\"determine_target_grade\", \"investment_plan_generator\")\n",
    "optimizer_builder.add_edge(\"investment_plan_generator\", \"evaluate_plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Routing\n",
    "\n",
    "To enable iterative refinement, we complete the reflection loop by adding a **conditional edge** from the evaluator node. This edge dynamically routes the workflow based on the evaluation outcome. The conditional edge uses a `lambda` function to invoke the `route_investment` decision logic. This logic compares the evaluator's assigned risk grade against the target risk profile, and checks how many iterations have already occurred.\n",
    "\n",
    "Depending on the result:\n",
    "\n",
    "- If the plan is **Accepted** (as in, the grade matches the target), the workflow proceeds to `END`.\n",
    "- If the plan is **Rejected + Feedback**, it loops back to the generator for refinement.\n",
    "- If the maximum number of iterations (for example, 5) is reached, the loop exits gracefully with the last attempt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add conditional edge for reflection\n",
    "optimizer_builder.add_conditional_edges(\n",
    "    \"evaluate_plan\",\n",
    "    lambda state: route_investment(state),\n",
    "    {\n",
    "        \"Accepted\": END,\n",
    "        \"Rejected + Feedback\": \"investment_plan_generator\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll compile our builder into a workflow that can take an investment profile and iteratively generate an investment plan for it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the workflow\n",
    "optimizer_workflow = optimizer_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Again, we'll use LangGraph's built in Mermaid charts to visualize the graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the graph\n",
    "display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing (Reflection)\n",
    "\n",
    "Finally, we'll test the workflow by invoking it with an example investment profile to see the end-to-end investment plan generation process in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the workflow with an example investor profile\n",
    "state = optimizer_workflow.invoke({\n",
    "    \"investor_profile\": (\n",
    "        \"Age: 29\\n\"\n",
    "        \"Salary: $110,000\\n\"\n",
    "        \"Assets: $40,000\\n\"\n",
    "        \"Goal: Achieve financial independence by age 45\\n\"\n",
    "        \"Risk tolerance: High\"\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the final returned state of the workflow below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_final_state(state: dict):\n",
    "    print(\"üéØ Final Investment Plan Summary\\n\" + \"=\"*40)\n",
    "    print(f\"\\nüìå Investor Profile:\\n{state['investor_profile']}\")\n",
    "    \n",
    "    print(\"\\nüìà Target Risk Grade:\", state['target_grade'])\n",
    "    print(\"üìä Final Assigned Grade:\", state['grade'])\n",
    "    print(\"üîÅ Iterations Taken:\", state['n'])\n",
    "\n",
    "    print(\"\\nüìù Evaluator Feedback:\\n\" + \"-\"*30)\n",
    "    print(state['feedback'])\n",
    "\n",
    "    print(\"\\nüìÉ Final Investment Plan:\\n\" + \"-\"*30)\n",
    "    print(state['investment_plan'])\n",
    "\n",
    "pretty_print_final_state(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab, we‚Äôve explored how **LangGraph** empowers you to build dynamic, agentic AI workflows. \n",
    "\n",
    "Throughout, we learned and applied important concepts such as:\n",
    "\n",
    "1. **Orchestrator‚ÄìWorker Pattern**  \n",
    "   - High-level planning with an orchestrator node  \n",
    "   - Parallel task execution by independent worker nodes  \n",
    "   - Automatic state merging via `Annotated` fields\n",
    "\n",
    "2. **Reflection Pattern**  \n",
    "   - Iterative generate‚Äìevaluate loops inspired by real investor personas  \n",
    "   - Structured feedback using Pydantic output parsers  \n",
    "   - Conditional routing to refine or accept results until they meet targets\n",
    "\n",
    "3. **State Management & Routing**  \n",
    "   - A shared `State` schema carrying context across nodes  \n",
    "   - Flexible fan-out and conditional edges (`Send`, `add_conditional_edges`)  \n",
    "   - Loop control via iteration counters and routing logic\n",
    "\n",
    "4. **Visualization & Testing**  \n",
    "   - Mermaid diagrams for graph structure inspection  \n",
    "   - Manual simulations and pretty-print utilities for debugging and presentation\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Check out [Watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Agentic+Design+Patterns+in+LangGraph-v1_1752612018)**: AI Agents for business to automate and handle simple tasks using orchestration.\n",
    "- **Extend the Patterns**: Try the optimizer (plan‚Äìevaluate) pattern or combine reflection with reinforcement learning.\n",
    "- **Domain Adaptation**: Swap out the grocery-list or investment example for customer support, legal drafting, or robotic control.\n",
    "- **Monitoring & Metrics**: Instrument your graph with logging, latency tracking, and success/failure metrics.\n",
    "- **Error Handling & Robustness**: Add retry policies, fallback nodes, and alerting for production readiness.\n",
    "\n",
    "By mastering these design patterns‚Äî**orchestration**, **evaluation**, **reflection**, and **routing**‚Äîyou can create truly **agentic systems** that plan, critique, adapt, and improve over time. \n",
    "\n",
    "#### Happy building!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) | Data Scientist @ IBM <br>\n",
    "[Wojciech \"Victor\" Fulmyk](https://author.skills.network/instructors/wojciech_fulmyk) | Data Scientist @ IBM <br>\n",
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana) | AI Software Developer @ IBM<br>\n",
    "[Joshua Zhou](https://author.skills.network/instructors/joshua_zhou) | Data Scientist Intern @ IBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for the changelog</summary>\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2025-06-25|0.1|Joseph|Initial version created|\n",
    "|2025-07-11|1.0|Joshua|First completed draft|\n",
    "|2025-07-22|1.1|Steve Ryan|ID review|\n",
    "|2025-07-22|1.2|Leah Hanson|QA review|\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright ¬© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "b77af27fa5f24ee7cc03d6b2bd8f8b824abd683df1d243d194cd42fdab910b93"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
